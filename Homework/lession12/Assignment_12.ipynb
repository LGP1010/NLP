{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?\n",
    "#### 自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?\n",
    "#### greedy search是每次预测取最大概率项；beam search每次预测取top n的概率项，beam search每一步都保留k个最有可能的结果，在每一步，基于之前的k个可能最优结果，继续搜索下一步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?\n",
    "#### 给定一组向量集合values，以及一个向量query，attention机制是一种根据该query计算values的加权求和的机制。attention的重点就是这个集合values中的每个value的“权值”的计算方法。attention机制就是一种根据某些规则或者某些额外信息（query）从向量表达集合（values）中抽取特定的向量进行加权组合（attention）的方法。备注：decoder的第t步的hidden state----st是query，encoder的hidden states是values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?\n",
    "#### 无法联系上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)\n",
    "#### ELMO的本质思想是：事先用语言模型在一个大的语料库上学习好词的word embedding，但此时的多义词仍然无法区分，后续接着用训练数据（去除标签）来fine-tuning 预训练好的ELMO 模型。ELMO 基于语言模型的，确切的来说是一个 Bidirectional language models，也是一个 Bidirectional LSTM结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?\n",
    "#### transformer具备rnn的优点，并行处理序列中的所有单词或符号，同时利用自注意力机制将上下文与较远的单词结合起来。通过并行处理所有单词，并让每个单词在多个处理步骤中注意到句子中的其他单词，Transformer 的训练速度比 RNN 快很多，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?\n",
    "#### layer normalizaiton是将词向量进行标准化，batch normalization是将词的一个维度进行标准化。可以对transformer学习过程中由于多词条embedding累加可能带来的“尺度”问题施加约束，相当于对表达每个词一词多义的空间施加了约束，有效降低模型方差。batch normalization也不具备这个功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?\n",
    "#### transformer模型的attention机制并没有包含位置信息，通过引入position encoding，传递每个词的位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?\n",
    "#### Self-Attention利用了Attention机制，计算每个单词与其他所有单词之间的关联，Multi-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?\n",
    "#### 多层Transformer的decoder的语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?\n",
    "#### GPT 采用两阶段过程，第一个阶段是利用语言模型进行预训练（无监督形式），第二阶段通过 Fine-tuning 的模式解决下游任务（监督模式下）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?\n",
    "#### 随机mask语料中15%的token，然后将masked token 位置输出的最终隐层向量送入softmax，来预测masked token。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?\n",
    "#### BERT输入表示。输入嵌入是token embeddings, segmentation embeddings 和position embeddings 的总和。而对于盖住词的特殊标记，在下游NLP任务中不存在。因此，为了和后续任务保持一致，作者按一定的比例在需要预测的词位置上输入原词或者输入某个随机的词。\n",
    "##### 有80%的概率用“[mask]”标记来替换——my dog is [MASK]\n",
    "##### 有10%的概率用随机采样的一个单词来替换——my dog is apple\n",
    "##### 有10%的概率不做替换——my dog is hairy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks.\n",
    "#### BERT将传统大量在下游具体NLP任务中做的操作转移到预训练词向量中，在获得使用BERT词向量后，最终只需在词向量上加简单的MLP或线性分类器即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT的预训练虽然仍然是以语言模型作为目标任务，采用的是单向的transformer结构，针对下游任务，需要将任务的网络结构改造成和GPT的网络结构是一样的；\n",
    "#### GPT-2依然沿用GPT单向transformer的模式，GPT-2去掉了fine-tuning层、增加数据集、增加网络参数、调整transformer：将layer normalization放到每个sub-block之前，并在最后一个Self-attention后再增加一个layer normalization。\n",
    "#### Bert和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。训练过程采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
